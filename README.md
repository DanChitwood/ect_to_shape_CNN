# ECT-to-shape CNN
A CNN for segmenting leaf shapes from their corresponding radial ECTs

[<img src="leaf_segmentation_figure.png">](https://github.com/DanChitwood/ect_to_shape_CNN/blob/main/ect_to_shape_CNN/outputs/figures/leaf_segmentation_figure.png)

## Data inputs  
The folders `Leafsnap` and `Transect` containing `.npy` files of leaf coordinate data from the following link should be placed in the folder `data`: https://figshare.com/articles/dataset/Modified_leaf_shape_contour_data/25435936

## Preparing radial ECTs and aligned leaf shape masks  
`0_radial_ect_and_masks.py`  
Raw leaf outline data, provided as 2D NumPy arrays in .npy files, was processed to generate input data for a convolutional neural network (CNN) and associated visualizations. The 0_radial_ect_and_masks.py script, located in the scripts/ directory, systematically located all .npy files by recursively scanning the data/ directory. For each leaf shape, a series of normalization steps were applied using the ect library: coordinates were centered at the origin, subjected to an internal transformation for consistent orientation, and scaled such that the maximum radial extent was 1. The radial Euler Characteristic Transform (ECT) was then computed for each normalized shape using 360 angular directions and 360 radial thresholds ranging from 0 to 1. The resulting ECT matrix was saved as a 256
times256 pixel grayscale PNG image in a polar coordinate system within the radial_ects/ subdirectory. Simultaneously, a corresponding 256
times256 pixel grayscale binary mask of the normalized leaf shape was generated, depicting the leaf as white pixels on a black background, and saved to the shape_masks/ subdirectory. For qualitative visual assessment, a combined visualization was also produced, displaying the inverted grayscale ECT on a white background with a superimposed black outline of the leaf shape, generated by plotting the normalized coordinates. These combined visualizations were saved as PNG files in the combined_viz/ subdirectory. All output directories are located under outputs/processed_leaf_data/. A metadata.csv file was produced to document each processed leaf, including its original data path, processing success, and the relative paths to all generated image files. Shapes failing to process due to invalid data format or geometric degeneration were noted in the metadata and skipped.

## Training a CNN to segment the shape of a leaf from corresponding radial ECT  
`1_leaf_segmentation.py`  
The inverse transformation from radial Euler Characteristic Transforms (ECTs) to leaf shape masks was approximated using a U-Net convolutional neural network (CNN) for semantic segmentation. The 1_leaf_segmentation.py script, located in the scripts/ directory, managed the training process. Input data consisted of ECT images as network inputs and corresponding binary shape masks as ground truth targets, both 256
times256 pixels. These images were sourced from the outputs/processed_leaf_data/radial_ects/ and outputs/processed_leaf_data/shape_masks/ subdirectories, respectively, with paths specified in outputs/processed_leaf_data/metadata.csv. A custom PyTorch Dataset class handled image loading and basic transformations, resizing all images to 256
times256 and converting them to torch.Tensor objects with pixel values scaled to [0.0,1.0]. The ground truth masks were binarized to strictly 0.0 or 1.0. The dataset was deterministically split into training (80), validation (10), and test (10) sets using a fixed random seed of 42 for reproducibility of the split. Data was loaded in batches of 16 using PyTorch DataLoaders with 2 worker processes.

The U-Net architecture comprised a contracting path (encoder) and an expansive path (decoder), each consisting of DoubleConv blocks (two 3
times3 convolutions, Batch Normalization, and ReLU activation). The encoder progressively downsampled the input feature maps using 2
times2 max pooling layers, increasing channel depth from 1 (input ECT) to 64,128,256,512, reaching 1024 channels at the bottleneck. The decoder upsampled feature maps using bilinear interpolation, concatenating them with corresponding feature maps from the encoder (skip connections) to recover spatial information, reducing channel depth back to 64. The final layer was a 1
times1 convolution to produce a single-channel output representing the segmentation logits. The network was trained for 50 epochs using the Adam optimizer with a learning rate of 1
times10 
−4
 . Binary Cross-Entropy with Logits Loss (nn.BCEWithLogitsLoss) was employed as the training objective, suitable for binary segmentation tasks, and the Dice coefficient was used as the primary evaluation metric, calculated after applying a sigmoid activation and binarizing predictions at a threshold of 0.5. The model achieving the highest Dice coefficient on the validation set was saved to outputs/models/best_unet_model.pth, and comprehensive training metrics (loss and Dice scores for training and validation) were logged to outputs/models/training_metrics.csv. All computations were performed on the available hardware (MPS or CUDA if available, otherwise CPU), ensuring full reproducibility via a global random seed set to 42.

## Visualizing prediction results  
`2_predict_and_visualize.py`  
The visualization of model predictions was performed using a dedicated Python script (2_predict_and_visualize.py). To ensure reproducibility of the specific set of samples displayed, a MANUAL_SEED_VIZ of 123 was used for random selection of leaves from the test dataset. The test dataset itself was consistent with the training phase, being generated by a random_split operation with a MANUAL_SEED of 42. A full-page figure of 8.5×11 inches was generated, comprising a 10×4 grid of leaf pairs (40 total leaves), each pair consisting of a radial ECT image and a segmentation overlay. For the radial ECT, the raw image data were inverted and a gamma correction (power of 2.0) was applied to enhance contrast, with the plot background set to white. A black outline of the ground truth leaf shape was overlaid on the radial ECT images to provide spatial context. The segmentation overlay panel depicted the true positives in white, false positives in magenta, false negatives in dodgerblue, and the background in gray. All images were uniformly resized to 256×256 pixels for display.
